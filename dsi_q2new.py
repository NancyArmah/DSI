# -*- coding: utf-8 -*-
"""DSI_Q2new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ZXMqGn5kv7m_NvVReExemTwWrPDU6W-
"""

import pandas as pd
import numpy as np

"""# Q.1"""

df1=pd.read_csv('data.tsv', sep='\t')

df1.head(10)

df2=pd.read_csv('data-2.tsv', sep='\t')

df2.head(10)

df3=pd.read_csv('data-3.tsv', sep='\t')

df3.head(10)

"""# Q.2"""

duplicate=df1[df1.duplicated()]
print(duplicate)

duplicate=df2[df2.duplicated()]
print(duplicate)

duplicate=df3[df3.duplicated()]
print(duplicate)

df1.duplicated().sum()

df2.duplicated().sum()

df3.duplicated().sum()

df2.rename(columns = {'titleId':'tconst'}, inplace = True)

df2.head()

"""# Q.3"""

#Inner Join
merged_df = pd.merge(pd.merge(df1,df2,on='tconst'),df3,on='tconst')

len(merged_df)

merged_df['titleType'].unique()

merged_df['titleType'].nunique()

"""# Q.4"""

df_new = merged_df[(merged_df['language'] == 'en') | (merged_df['region'] == 'US') & (merged_df['titleType']== 'movie') ]

"""# Q.5"""

df_new.head()

df_new.loc[df_new['log10Votes'] == np.log10(df_new['numVotes'])]

"""# Q.6"""

df_new['genres']=df_new['genres'].str.lower()

df_new.head()

df_new.groupby('genres')['log10Votes'].mean().nlargest(10)

df_new.groupby('genres')['averageRating'].mean().nlargest(10)

"""## Q.7"""

df_new.groupby('averageRating')
df_new.plot.scatter(x='averageRating', y='log10Votes')

"""## Q.8"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

X = df_new.iloc[:, 1].values.reshape(-1, 1)
y = df_new.iloc[:, -1].values.reshape(-1, 1)

print(X)

print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)

plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.title('averageRating vs log10Votes (Training Set)')
plt.xlabel('averageRating')
plt.ylabel('log10Votes')
plt.show()

plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.title('averageRating vs log10Votes (Test Set)')
plt.xlabel('averageRating')
plt.ylabel('log10Votes')
plt.show()

print(regressor.intercept_)
print(regressor.coef_)

"""Linear Regression Using Scipy"""

from scipy import stats
from scipy.stats import linregress

x = df_new.iloc[:, 1].values.reshape(-1, 1)
y = df_new.iloc[:, -1].values.reshape(-1, 1)

slope, intercept, r_value, p_value, stderr = linregress(x, y)

plt.scatter(x,y, color="red", marker="o", label="averageRating vs log10Votes")
 
 
y_pred = intercept + slope*x
plt.plot(x,y_pred, color="green", label="Fitted line")
 
plt.legend(loc='best')
plt.xlabel('averageRating') 
plt.ylabel('log10Votes')



"""Linear Regression using Pytorch"""

import torch
import torch.nn as nn
from torch.autograd import Variable

x_values = df_new.iloc[:, 1].values
x_train = np.array(x_values, dtype=np.float32)
x_train = x_train.reshape(-1, 1)

y_values = df_new.iloc[:, -1].values
y_train = np.array(y_values, dtype=np.float32)
y_train = y_train.reshape(-1, 1)

class linearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(linearRegression, self).__init__()
        self.linear = torch.nn.Linear(inputSize, outputSize)

    def forward(self, x):
        out = self.linear(x)
        return out

inputDim = 1        
outputDim = 1       
learningRate = 0.01 
epochs = 100

model = linearRegression(inputDim, outputDim)
if torch.cuda.is_available():
    model.cuda()

criterion = torch.nn.MSELoss() 
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)

for epoch in range(epochs):
    # Converting inputs and labels to Variable
    if torch.cuda.is_available():
        inputs = Variable(torch.from_numpy(x_train).cuda())
        labels = Variable(torch.from_numpy(y_train).cuda())
    else:
        inputs = Variable(torch.from_numpy(x_train))
        labels = Variable(torch.from_numpy(y_train))

    optimizer.zero_grad()

    outputs = model(inputs)

    loss = criterion(outputs, labels)
    print(loss)
    loss.backward()

    optimizer.step()

    print('epoch {}, loss {}'.format(epoch, loss.item()))

with torch.no_grad(): # we don't need gradients in the testing phase
    if torch.cuda.is_available():
        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()
    else:
        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()
    print(predicted)

plt.clf()
plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)
plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)
plt.legend(loc='best')
plt.show()